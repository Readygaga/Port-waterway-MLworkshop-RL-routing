{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete, Sequence\n",
    "from gymnasium.wrappers import FlattenObservation\n",
    "\n",
    "# import helpers\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import tensorboard\n",
    "import pygame\n",
    "import pylab\n",
    "from pygame.locals import *\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# import stable_baselines3\n",
    "from stable_baselines3 import PPO, SAC, A2C\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# import sb3_contrib\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "# render related\n",
    "\n",
    "\n",
    "# routing related\n",
    "import networkx as nx\n",
    "from statistics import mode\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import main\n",
    "import NSGA\n",
    "from main import Graph, FishGround, FishFactory, WayPoint, Start, End, Position, great_circle_distance, \\\n",
    "    scgraph_distance, pathRisk, pathCatch, pathGain, pathFuel, pathDistance, Ship, dynamic_routing\n",
    "\n",
    "\n",
    "# Create a class function for making training environment\n",
    "# Objective: Get the most catch\n",
    "\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "graph1 = Graph(G)\n",
    "loc1 = FishGround(name='FG1', location=Position(30, 15), fishstock={'Cod': {'quantity': 100, 'timewindow': (1, 2)}})\n",
    "loc2 = FishGround(name='FG2', location=Position(30, 50), fishstock={'Cod': {'quantity': 500, 'timewindow': (1, 2)}})\n",
    "loc3 = FishGround(name='FG3', location=Position(60, 50), fishstock={'Cod': {'quantity': 300, 'timewindow': (1, 2)}})\n",
    "start = Start(name='S', location=Position(10, 10))\n",
    "end = End(name='E', location=Position(80, 80))\n",
    "ff1 = FishFactory(name='FF1', location=Position(50, 60))\n",
    "ff2 = FishFactory(name='FF2', location=Position(70, 50))\n",
    "\n",
    "locations = [loc1, loc2, loc3, start, end, ff1, ff2]\n",
    "graph1.addlocations_2(locations=locations)\n",
    "ship = Ship(targeted_fish='Cod', targeted_quantity=1000, weight=1000)\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "import matplotlib.backends.backend_agg as agg\n",
    "\n",
    "\n",
    "class Routing(Env):\n",
    "    def __init__(self, Graph: Graph, Ship: Ship, start):\n",
    "        self.ship = copy.deepcopy(Ship)\n",
    "        self.graph = Graph\n",
    "        self.Footprint = nx.DiGraph()\n",
    "        self.Footprint.add_node(start)\n",
    "        nodeattr = self.graph.graph.nodes[start].copy()\n",
    "        nx.set_node_attributes(self.Footprint, {start: nodeattr})\n",
    "        self.successorsLst = nx.dfs_successors(G=self.graph.graph, source=start, depth_limit=1)[start]\n",
    "        self.action_space = Discrete(7)\n",
    "        # old_action_space=Dict({'location_to_go': Discrete(len(self.successorsLst)), 'speed': Box(low=0, high=30, shape=(1,)), 'duration': Box(0, 10, shape=(1,))})\n",
    "        self.observation_space = Dict({'route': Discrete(7),\n",
    "                                       'pathCatch': Box(low=0, high=30000, shape=(1,), dtype=float)})\n",
    "        self.current_place = start\n",
    "        locations.index(start)\n",
    "        self.state = {'route': locations.index(start), 'pathCatch': 0}\n",
    "        self.route_state = {'footprint': self.Footprint, 'route': [start]}\n",
    "        self.previous_place = None\n",
    "        self.route_length = 10\n",
    "\n",
    "    def step(self, action):\n",
    "        # print(action[0])\n",
    "        #print('successors:', [loc.name for loc in self.successorsLst])\n",
    "        location = locations[action]\n",
    "        # print(location.name)\n",
    "        self.previous_place = self.current_place\n",
    "        # Update state, including Footprint, route, speed and duration\n",
    "        self.current_place = location\n",
    "        self.route_state['route'].append(self.current_place)\n",
    "        nodeattr = self.graph.graph.nodes[self.current_place].copy()\n",
    "        self.Footprint.add_node(self.current_place)\n",
    "        nx.set_node_attributes(self.Footprint, {self.current_place: nodeattr})\n",
    "        self.Footprint.add_edge(self.previous_place, self.current_place)\n",
    "        edgeattr = self.graph.graph.edges[(self.previous_place, self.current_place)].copy()\n",
    "        nx.set_edge_attributes(self.Footprint, {(self.previous_place, self.current_place): edgeattr})\n",
    "        #print('current_place', self.current_place.name)\n",
    "        # rewards\n",
    "        if isinstance(self.current_place, FishGround):\n",
    "            fishtype = self.ship.targeted_fish\n",
    "            newcatch = self.ship.update_catch(self.current_place)\n",
    "            stock = self.current_place.fishstock\n",
    "            stock[fishtype]['quantity'] -= newcatch\n",
    "            self.current_place.update_stock(stock)\n",
    "            self.graph.update_node_attribute(self.current_place, 'fishstock', stock)\n",
    "        else:\n",
    "            newcatch = 0\n",
    "        self.state = {'route': action, 'pathCatch': newcatch}\n",
    "        reward = newcatch\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        self.route_length -= 1\n",
    "        # conditions to end episode\n",
    "        if isinstance(self.current_place, End):\n",
    "            #reward = reward\n",
    "            done = True\n",
    "        elif self.route_length <= 0:\n",
    "            #reward = -1000\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        # print('steps', self.route_length)\n",
    "        # print('obs', self.state)\n",
    "\n",
    "        return self.state, reward, done, False, info\n",
    "\n",
    "    def render(self):\n",
    "        fig = pylab.figure(figsize=[4, 4],  # Inches\n",
    "                           dpi=200,  # 100 dots per inch, so the resulting buffer is 400x400 pixels\n",
    "                           )\n",
    "        fig.gca()\n",
    "        self.graph.plot_graph()\n",
    "        pos = {n: [n.position[1], n.position[0]] for n in list(self.Footprint.nodes())}\n",
    "        nx.draw_networkx(self.Footprint, with_labels=False, pos=pos, node_color='red', edge_color='red',\n",
    "                         font_color='red')\n",
    "        canvas = agg.FigureCanvasAgg(fig)\n",
    "        canvas.draw()\n",
    "        renderer = canvas.get_renderer()\n",
    "        raw_data = renderer.tostring_argb()\n",
    "        window = pygame.display.set_mode((800, 800), DOUBLEBUF)\n",
    "        screen = pygame.display.get_surface()\n",
    "        size = canvas.get_width_height()\n",
    "        surf = pygame.image.fromstring(raw_data, size, \"ARGB\")\n",
    "        screen.blit(surf, (0, 0))\n",
    "        pygame.display.flip()\n",
    "\n",
    "    def get_obs(self):\n",
    "        pass\n",
    "\n",
    "    def get_info(self):\n",
    "        self.info = {}\n",
    "        return self.info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        loc1.fishstock['Cod']['quantity'] = 100\n",
    "        loc2.fishstock['Cod']['quantity'] = 500\n",
    "        loc3.fishstock['Cod']['quantity'] = 300\n",
    "        self.ship = copy.deepcopy(ship)\n",
    "        self.Footprint = nx.DiGraph()\n",
    "        self.Footprint.add_node(start)\n",
    "        nodeattr = self.graph.graph.nodes[start].copy()\n",
    "        nx.set_node_attributes(self.Footprint, {start: nodeattr})\n",
    "        self.successorsLst = nx.dfs_successors(G=self.graph.graph, source=start, depth_limit=1)[start]\n",
    "        self.action_space = Discrete(7)\n",
    "        self.observation_space = Dict({'route': Discrete(7), 'pathCatch': Box(low=0, high=30000, shape=(1,), dtype=float)})\n",
    "        self.current_place = start\n",
    "        self.route_state = {'footprint': self.Footprint, 'route': [start]}\n",
    "        self.previous_place = None\n",
    "        self.route_length = 10\n",
    "        self.state = {'route': locations.index(start), 'pathCatch': 0}\n",
    "        info = self.get_info()\n",
    "        return self.state, info\n",
    "\n",
    "    def retrieve_location(self, action):\n",
    "        location = self.successorsLst[action[0]]\n",
    "        return\n",
    "\n",
    "    def action_masks(self):\n",
    "        if not isinstance(self.current_place, End):\n",
    "            successors = nx.dfs_successors(G=self.graph.graph, source=self.current_place, depth_limit=1)\n",
    "            #print('current_place', self.current_place.name)\n",
    "            self.successorsLst = successors[self.current_place]\n",
    "            # code below exclude fish ground where its stock is 0.\n",
    "            locLst = []\n",
    "            for loc in self.successorsLst:\n",
    "                if isinstance(loc, FishGround):\n",
    "                    if loc.fishstock['Cod']['quantity'] <= 0:\n",
    "                        locLst.append(loc)\n",
    "            newLst = list(filter(lambda loc: loc not in locLst, self.successorsLst))\n",
    "            self.successorsLst = newLst\n",
    "            location_mask = [1 if loc in newLst else 0 for loc in locations]\n",
    "        else:\n",
    "            successors = nx.dfs_successors(G=self.graph.graph, source=start, depth_limit=1)\n",
    "            self.successorsLst = successors[start]\n",
    "            location_mask = [1 if loc in self.successorsLst else 0 for loc in locations]\n",
    "        #print('location mask:', location_mask)\n",
    "        location_mask = np.array(location_mask, dtype=np.int8)\n",
    "        action_mask = location_mask\n",
    "        #print(action_mask)\n",
    "        return action_mask\n",
    "\n",
    "def mask_fn(env):\n",
    "    return env.action_masks()\n",
    "envR = Routing(Graph=graph1, Ship=ship, start=start)\n",
    "envR = ActionMasker(envR, mask_fn)\n",
    "\n",
    "#Specify location for log and model file\n",
    "Objective = 'Obj2'\n",
    "\n",
    "#Test training environment\n",
    "# episodes = 5\n",
    "# score_list = []\n",
    "# for episode in range(1, episodes +1):\n",
    "#     print('episode:{}'.format(episode))\n",
    "#     done = False\n",
    "#     obs, _ = envR.reset()\n",
    "#     envR.render()\n",
    "#     pygame.time.delay(1000)\n",
    "#     score = 0\n",
    "#     length = 10\n",
    "#     while not done:\n",
    "#         action = envR.action_space.sample(mask=envR.action_masks())\n",
    "#         print('action chosen:', action)\n",
    "#         obs, reward, done, truncated, info = envR.step(action)\n",
    "#         envR.render()\n",
    "#         pygame.time.delay(1000)\n",
    "#         score += reward\n",
    "#     score_list.append(reward)\n",
    "#     print('episode:{} reward:{}'.format(episode, reward))\n",
    "# print(np.mean(score_list))\n",
    "# print(np.std(score_list))\n",
    "# envR.close()\n",
    "\n",
    "# #Train an agent\n",
    "# timesteps = [1000000] # 20000, 100000, 1000000, 3000000, 5000000, 10000000\n",
    "# for timestep in timesteps:\n",
    "#     total_timesteps=timestep\n",
    "#     start_time = time.time()\n",
    "#     log_path = os.path.join('Training', 'Logs', Objective, f'{Objective}_{total_timesteps}stepModel')\n",
    "#     model_path = os.path.join('Training', 'Model', Objective, f'{Objective}_{total_timesteps}stepModel')\n",
    "#     model = MaskablePPO(\"MultiInputPolicy\", envR, verbose=1, tensorboard_log=log_path, seed=100)\n",
    "#     model.learn(total_timesteps=total_timesteps)\n",
    "#     finish_time = time.time()\n",
    "#     time_used = finish_time-start_time\n",
    "#     print('Time spent for training:', time_used)\n",
    "#     #save the trained model\n",
    "#     model.save(model_path)\n",
    "#\n",
    "# # Load a trained agent\n",
    "# total_timesteps=5000000\n",
    "# model_path = os.path.join('Training', 'Model', Objective, f'{Objective}_{total_timesteps}stepModel')\n",
    "# model = MaskablePPO.load(model_path, env=envR)\n",
    "#\n",
    "#\n",
    "# # Test the trained agent\n",
    "# vec_env = model.get_env()\n",
    "# obs  = vec_env.reset()\n",
    "# print('Initial observation',obs)\n",
    "# action, _ = model.predict(obs, action_masks=envR.action_masks())\n",
    "# print('chosen action by agent according to observation', action)\n",
    "# obs, reward, done, truncated = vec_env.step(action)\n",
    "# print('step output',obs, reward, done, truncated)\n",
    "#\n",
    "# # Run some episodes to see how the trained agent performs\n",
    "# episodes = 10\n",
    "# score_list = []\n",
    "# for episode in range(1, episodes + 1):\n",
    "#     print('episode:{}'.format(episode))\n",
    "#     done = False\n",
    "#     obs = vec_env.reset()\n",
    "#     envR.render()\n",
    "#     pygame.time.delay(100)\n",
    "#     print('Observation output from initial',obs)\n",
    "#     score = 0\n",
    "#     length = 10\n",
    "#     while not done:\n",
    "#         print('input_obs', obs)\n",
    "#         action, _ = model.predict(obs, action_masks=envR.action_masks()) #, action_masks=envR.action_masks()\n",
    "#         print('action chosen:', action)\n",
    "#         obs, reward, done, truncated = vec_env.step(action)\n",
    "#         envR.render()\n",
    "#         pygame.time.delay(100)\n",
    "#         print('output_obs', obs, reward, done, truncated) #observation is the same as state\n",
    "#         score += reward\n",
    "#     score_list.append(reward)\n",
    "# print(np.mean(score_list))\n",
    "# print(np.std(score_list))\n",
    "# envR.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
